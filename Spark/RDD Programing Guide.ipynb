{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "1. Every spark application consists of a driver program that runs\n",
    "the users main function and excutes various parallel operatoins on\n",
    "a cluster.\n",
    "2. RDD is a collection of elements partitioned across the nodes of\n",
    "cluster that can be operated on in parallel.\n",
    "3. User can ask Spark to persist an RDD in memory.\n",
    "4. RDD automatically reconver from node failures\n",
    "5. Shared variables can be used in parallel operations.\n",
    "6. When spark runs a function in parallel as a set of tasks on \n",
    "different nodes, it ships a copy of each variable used in the \n",
    "function to each tasks.\n",
    "7. Spark supports tow types of share variables: broadcast variables\n",
    "    , which can be used to cache a value in mememory on all nodes,\n",
    "    and accumulators, which are varibles that are only 'added' to\n",
    "    such as counters and sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "## The first thing a Spark program must do is to create a SparkContext\n",
    "## which tells Spark how to access a cluster. To create a SparkContext\n",
    "## you first need to build a SparkConf object that contains information\n",
    "## about your application\n",
    "conf = SparkConf().setAppName(\"RDD programing guide\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD can be created in two ways\n",
    "1. parallelize method on an existing itertable or collection in your\n",
    "driver program\n",
    "2. referencing a dataset in an external storage system such as shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## parallelize collections\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "## wec can perform parallel operation after distribute the collection\n",
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External DataSets\n",
    "## Spark Supports text files, SequenceFiles, and any other Hadoop\n",
    "## InputFormat\n",
    "distFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writable Support\n",
    "When reading an RDD of key-value pairs from squenceFile, the Pyspark\n",
    "SequenceFile support Loads an RDD of key-value pairs within java, \n",
    "converts Writables to base Java types, and pickles the resulting java\n",
    "objects using Pyrolite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,4)).map(lambda x: (x, \"a\" * x))\n",
    "## When saving an RDD of key-value pairs to SequenceFile, Pyspark\n",
    "## does the reverse. It unpickles Python objects into Java objects\n",
    "## and then converts the to Writables.\n",
    "rdd.saveAsSequenceFile(\"./saveSequence\")\n",
    "sorted(sc.sequenceFile(\"./saveSequence\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Other Hadoop Input/Output Formats\n",
    "PySpark can also read any Hadoop InputFormat or write any Hadoop\n",
    "OutputFormat, for both 'new' and 'old' Hadoop MapReduce APIs. If\n",
    "required, a Hadoop configuration can be passed in as a Python dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have custom serialized binary data (such as loading data from\n",
    "Cassandra / Hbase), then you will first need to transform that data\n",
    "on the Scala/java side to something which can be handled by Pyrolite's'\n",
    "pickler. \n",
    "\n",
    "A Converter trait is provided for this. Simply extend this trait and\n",
    "implement your transformation code in the convert method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "RDDs support two types of operations:\n",
    "1. transformation\n",
    "    create a new dataset from an existing one\n",
    "2. actions\n",
    "    return a value to the driver program after running a computation\n",
    "    on the dataset.\n",
    "transformations are lazy.\n",
    "\n",
    "However you may also persist an RDD in memory or disk or replicated across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basics\n",
    "lines = sc.textFile(\"README.md\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "\n",
    "## if you wanted to use lineLengths again later, we could add:\n",
    "## rdd.persist() method will persist the result in memory\n",
    "lineLengths.persist()\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Functions to Spark\n",
    "Spark recommend three ways to pass functions in the driver program\n",
    "to run on the cluster\n",
    "1. Lambda expressions\n",
    "2. Local defs\n",
    "3. Top-level functions in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "sc.textFile(\"README.md\").map(myFunc).reduce(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding closures\n",
    "One of the harder things about Spark is understanding the scope and\n",
    "life cycle of variables and methods when executing code across cluster.\n",
    "RDD operations that modify variables outside of their scope can be \n",
    "a frequent source of confusion. In the example below we ll look at code\n",
    "that uses foreach() to increment a counter, but similar issues can \n",
    "occur for other operations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value: 0\n"
     ]
    }
   ],
   "source": [
    "## Example\n",
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this !!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "\n",
    "rdd.foreach(increment_counter)\n",
    "print(\"Counter value:\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local vs. cluster modes\n",
    "The behavior of the above code is undefined, and may not work as inteded\n",
    "To execute jobs, Spark breaks up the processing of RDD oeprations into\n",
    "tasks, each of which is executed by an executor. Prior to execution,\n",
    "Spark computes the task's closure. The closure is those variables\n",
    "and methods which must be visible for the executor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
