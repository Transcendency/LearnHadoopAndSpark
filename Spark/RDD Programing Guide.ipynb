{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "1. Every spark application consists of a driver program that runs\n",
    "the users main function and excutes various parallel operatoins on\n",
    "a cluster.\n",
    "2. RDD is a collection of elements partitioned across the nodes of\n",
    "cluster that can be operated on in parallel.\n",
    "3. User can ask Spark to persist an RDD in memory.\n",
    "4. RDD automatically reconver from node failures\n",
    "5. Shared variables can be used in parallel operations.\n",
    "6. When spark runs a function in parallel as a set of tasks on \n",
    "different nodes, it ships a copy of each variable used in the \n",
    "function to each tasks.\n",
    "7. Spark supports tow types of share variables: broadcast variables\n",
    "    , which can be used to cache a value in mememory on all nodes,\n",
    "    and accumulators, which are varibles that are only 'added' to\n",
    "    such as counters and sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "## The first thing a Spark program must do is to create a SparkContext\n",
    "## which tells Spark how to access a cluster. To create a SparkContext\n",
    "## you first need to build a SparkConf object that contains information\n",
    "## about your application\n",
    "conf = SparkConf().setAppName(\"RDD programing guide\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD can be created in two ways\n",
    "1. parallelize method on an existing itertable or collection in your\n",
    "driver program\n",
    "2. referencing a dataset in an external storage system such as shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## parallelize collections\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "## wec can perform parallel operation after distribute the collection\n",
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External DataSets\n",
    "## Spark Supports text files, SequenceFiles, and any other Hadoop\n",
    "## InputFormat\n",
    "distFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writable Support\n",
    "When reading an RDD of key-value pairs from squenceFile, the Pyspark\n",
    "SequenceFile support Loads an RDD of key-value pairs within java, \n",
    "converts Writables to base Java types, and pickles the resulting java\n",
    "objects using Pyrolite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,4)).map(lambda x: (x, \"a\" * x))\n",
    "## When saving an RDD of key-value pairs to SequenceFile, Pyspark\n",
    "## does the reverse. It unpickles Python objects into Java objects\n",
    "## and then converts the to Writables.\n",
    "rdd.saveAsSequenceFile(\"./saveSequence\")\n",
    "sorted(sc.sequenceFile(\"./saveSequence\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Other Hadoop Input/Output Formats\n",
    "PySpark can also read any Hadoop InputFormat or write any Hadoop\n",
    "OutputFormat, for both 'new' and 'old' Hadoop MapReduce APIs. If\n",
    "required, a Hadoop configuration can be passed in as a Python dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have custom serialized binary data (such as loading data from\n",
    "Cassandra / Hbase), then you will first need to transform that data\n",
    "on the Scala/java side to something which can be handled by Pyrolite's'\n",
    "pickler. \n",
    "\n",
    "A Converter trait is provided for this. Simply extend this trait and\n",
    "implement your transformation code in the convert method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "RDDs support two types of operations:\n",
    "1. transformation\n",
    "    create a new dataset from an existing one\n",
    "2. actions\n",
    "    return a value to the driver program after running a computation\n",
    "    on the dataset.\n",
    "transformations are lazy.\n",
    "\n",
    "However you may also persist an RDD in memory or disk or replicated across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basics\n",
    "lines = sc.textFile(\"README.md\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "\n",
    "## if you wanted to use lineLengths again later, we could add:\n",
    "## rdd.persist() method will persist the result in memory\n",
    "lineLengths.persist()\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Functions to Spark\n",
    "Spark recommend three ways to pass functions in the driver program\n",
    "to run on the cluster\n",
    "1. Lambda expressions\n",
    "2. Local defs\n",
    "3. Top-level functions in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "sc.textFile(\"README.md\").map(myFunc).reduce(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding closures\n",
    "One of the harder things about Spark is understanding the scope and\n",
    "life cycle of variables and methods when executing code across cluster.\n",
    "RDD operations that modify variables outside of their scope can be \n",
    "a frequent source of confusion. In the example below we ll look at code\n",
    "that uses foreach() to increment a counter, but similar issues can \n",
    "occur for other operations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value: 0\n"
     ]
    }
   ],
   "source": [
    "## Example\n",
    "counter = 0\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "# Wrong: Don't do this !!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "\n",
    "rdd.foreach(increment_counter)\n",
    "print(\"Counter value:\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local vs. cluster modes\n",
    "The behavior of the above code is undefined, and may not work as inteded\n",
    "To execute jobs, Spark breaks up the processing of RDD oeprations into\n",
    "tasks, each of which is executed by an executor. Prior to execution,\n",
    "Spark computes the task's closure. The closure is those variables\n",
    "and methods which must be visible for the executor to perform its\n",
    "computations on the RDD (in this case foreach()). This closure is\n",
    "serialized and sent to each excutor.\n",
    "\n",
    "The variables within the closure sent to each excutor are now copies and thus, when counter is referenced within the foreach function, it's no longer the counter on the driver node. There is still a counter in the memory of the driver node. There is still a counter\n",
    "in the memory of the driver node but this is no longer visible to\n",
    "the exectutors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
    "\n",
    "### Printing elements of an RDD\n",
    "Another common idiom is attempting to print out the elements of an\n",
    "RDD using rdd.foreach(println) or rdd.map(println). On a single\n",
    "machine, this will generate the expected output and print all the\n",
    "RDD's elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor's stdout\n",
    "instead, not the one on the driver, so stdout on the driver won't \n",
    "show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). THis can cause the driver to run\n",
    "out of the memory, though, becuase collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map(func):\n",
    "Return a new distributed dataset formed by passing each element of\n",
    "the source throught a function func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter(func): Return a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "\n",
    "flatMap(func): Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
    "\n",
    "mapPartition(func): Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T>\n",
    "=> Iterator<U> when running on an RDD of type T.\n",
    "    \n",
    "mapPartitionsWithIndex(func): Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator<T>) => Iterator<U>\n",
    "when running on an RDD of type T.\n",
    "    \n",
    "Sample(WithReplacement, fraction, seed)\n",
    "union(otherDataset)\n",
    "intersection(otherDataset)\n",
    "distinct([numPartitions])\n",
    "\n",
    "groupByKey([numPartitions]): When called on a dataset of (K,V) pairs\n",
    ", returns a dataset of (K, Iterable<V>) pairs.\n",
    "\n",
    "reduceByKey(func, [numPartions]): When called on a dataset of (K,V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type(V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument\n",
    "\n",
    "aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])\n",
    "\n",
    "sortBykey([ascending], [numPartitions])\n",
    "join(otherDataset, [numPartitions])\n",
    "cogroup(otherDataset, [numPartitions])\n",
    "\n",
    "## Actions\n",
    "reduce(func)\n",
    "collect()\n",
    "count()\n",
    "first()\n",
    "take(n)\n",
    "takeSample(withReplacement, num, [seed])\n",
    "takeOrdered(n, [ordering])\n",
    "saveAsTextFile(path)\n",
    "saveAsSequenceFile(path)\n",
    "saveAsObjectFile(paht)\n",
    "countByKey()\n",
    "foreach(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use sequence file\n",
    "First we should understand what problems does the SequenceFile try to solve, and then how can SequenceFile help to solve the problems.\n",
    "\n",
    "### In HDFS\n",
    "SequenceFile is one of the solutions to small file problem in Hadoop.\n",
    "Small file is significantly smaller than the HDFS block size(128MB).\n",
    "Each file, directory, block in HDFS is represented as object and occupies 150 bytes.\n",
    "10 million files, would use about 3 gigabytes of memory of NameNode.\n",
    "A billion files is not feasible.\n",
    "### In MapReduce\n",
    "Map tasks usually process a block of input at a time (using the default FileInputFormat).\n",
    "\n",
    "The more the number of files is, the more number of Map task need and the job time can be much more slower.\n",
    "\n",
    "### Small file scenarios\n",
    "The files are pieces of a larger logical file.\n",
    "The files are inherently small, for example, images.\n",
    "#### These two cases require different solutions.\n",
    "\n",
    "For first one, write a program to concatenate the small files together.(see Nathan Marz’s post about a tool called the Consolidator which does exactly this)\n",
    "For the second one, some kind of container is needed to group the files in some way.\n",
    "### Solutions in Hadoop\n",
    "#### HAR files\n",
    "\n",
    "HAR(Hadoop Archives) were introduced to alleviate the problem of lots of files putting pressure on the namenode’s memory.\n",
    "HARs are probably best used purely for archival purposes.\n",
    "#### SequenceFile\n",
    "\n",
    "The concept of SequenceFile is to put each small file to a larger single file.\n",
    "For example, suppose there are 10,000 100KB files, then we can write a program to put them into a single SequenceFile like below, where you can use filename to be the key and content to be the value.\n",
    "\n",
    "SequenceFile File Layout http://img.blog.csdn.net/20151213123516719\n",
    "\n",
    "Some benefits:\n",
    "\n",
    "A smaller number of memory needed on NameNode. Continue with the 10,000 100KB files example,\n",
    "Before using SequenceFile, 10,000 objects occupy about 4.5MB of RAM in NameNode.\n",
    "After using SequenceFile, 1GB SequenceFile with 8 HDFS blocks, these objects occupy about 3.6KB of RAM in NameNode.\n",
    "SequenceFile is splittable, so is suitable for MapReduce.\n",
    "SequenceFile is compression supported.\n",
    "Supported Compressions, the file structure depends on the compression type.\n",
    "\n",
    "Uncompressed\n",
    "Record-Compressed: Compresses each record as it’s added to the file. record_compress_seq http://img.blog.csdn.net/20151213182753789\n",
    "\n",
    "Block-Compressed 这里写图片描述 http://img.blog.csdn.net/20151213183017236\n",
    "\n",
    "Waits until data reaches block size to compress.\n",
    "Block compression provide better compression ratio than Record compression.\n",
    "Block compression is generally the preferred option when using SequenceFile.\n",
    "Block here is unrelated to HDFS or filesystem block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
