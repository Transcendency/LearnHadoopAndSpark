{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "1. Every spark application consists of a driver program that runs\n",
    "the users main function and excutes various parallel operatoins on\n",
    "a cluster.\n",
    "2. RDD is a collection of elements partitioned across the nodes of\n",
    "cluster that can be operated on in parallel.\n",
    "3. User can ask Spark to persist an RDD in memory.\n",
    "4. RDD automatically reconver from node failures\n",
    "5. Shared variables can be used in parallel operations.\n",
    "6. When spark runs a function in parallel as a set of tasks on \n",
    "different nodes, it ships a copy of each variable used in the \n",
    "function to each tasks.\n",
    "7. Spark supports tow types of share variables: broadcast variables\n",
    "    , which can be used to cache a value in mememory on all nodes,\n",
    "    and accumulators, which are varibles that are only 'added' to\n",
    "    such as counters and sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "## The first thing a Spark program must do is to create a SparkContext\n",
    "## which tells Spark how to access a cluster. To create a SparkContext\n",
    "## you first need to build a SparkConf object that contains information\n",
    "## about your application\n",
    "conf = SparkConf().setAppName(\"RDD programing guide\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD can be created in two ways\n",
    "1. parallelize method on an existing itertable or collection in your\n",
    "driver program\n",
    "2. referencing a dataset in an external storage system such as shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## parallelize collections\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "## wec can perform parallel operation after distribute the collection\n",
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External DataSets\n",
    "## Spark Supports text files, SequenceFiles, and any other Hadoop\n",
    "## InputFormat\n",
    "distFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
