CREATE TABLE table1
(col1 STRING,
 col2 INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';
[Internal table (folders deleted when table is dropped); Default location (/hive/warehouse/table1)]


CREATE TABLE table2
(col1 STRING,
 col2 INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';
STORED AS TEXTFILE LOCATION '/data/table2';
[Stored in a custom folder(but still internal, so the folder is deleted when table is dropped)] 

CREATE EXTERNAL TABLE table3
(col1 STRING,
 col2 INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';
STORED AS TEXTFILE LOCATION '/data/table2';
[External table (folders and files are left intact in Azure blob when the table is dropped)]

Hive data types:
- Numeric
	- Intefers: TINYINT, SMALLINT, INT, BIGINT
	- Fractinoal: FLOAT, DOUBLE, DECIMAL
- Character
	- STRING, VARCHAR, CHAR
- Date/Time
	-TIMESTAMP
	-DATE
- Specail
	- BOOLEAN, BINARY, ARRAY, MAP, STRUCT, UNIONTYPE

Save data files in table folders (or create table on existing files!)
- put myfile.txt /data/table1
Use the LOAD statement
- LOAD DATA [LOCAL] INPATH '/data/source' INTO TABLE MyTable;
Use the INSERT Statement (not insert records but insert existing tables or the results of query)
- INSERT INTO TABLE Table2 SELECT Col1, UPPER(col2), FROM Table1;
Use a CREATE TABLE AS SELECT (CTAS) statement
- CREATE TABLE Table3 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' SOTRED AS TEXTFILE LOCATION
'/data/summarytable' AS SELECT Col1, SUM(Col2) As Total FROM Table1 GROUP By Col1


HIVE DEMO
CREATE EXTERNAL TABLE staged_log
(level STRING,
datetime STRING,
source STRING,
event_id INT,
category STRING,
details STRING)
ROW FORMAT DELIMITED FIELDS TRMINATED BY '\t'
STORED AS TEXTFILE LOCATION '/data/staged_log';

SELECT * FROM staged_log;

SHOW TABLES;

LOAD DATA INPATH '/data/system_log.txt' INTO TABLE staged_log;

SELECT * FROM staged_log;

CREATE TABLE system_log
(level STRING,
datetime STRING,
source STRING,
event_id INT,
category STRING,
details STRING);

INSERT INTO TABLE system_log
SELECT * FROM staged_log
WHERE event_id IS NOT NULL;

SELECT * FROM system_log;

hdfs dfs -ls /data
hdfs dfs -ls /hive/warehouse

CREATE TABLE error_log
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STOREDS AS TEXTFILE LOCATION '/data/error_log'
AS
SELECT datetime, source, event_id, details
FROM system_log
WHERE level = 'Error';

- Query data using the SELECT HiveQL Statement
SELECT Col1, SUM(Col2) AS TotalCol2
FROM MyTable
WHERE Col3 = 'ABC' AND Col4 < 10
GROUP BY Col1
ORDER BY Col4;

Hive translates the query into jobs and applies the table schema to the underlying 
data files
Views are named queries that abstract underlying tables
CREATE VIEW v_SummarizaedData
AS
SELECT col1, SUM(col2) AS TotalCol2
FROM mytable
GROUP BY col1;

SELECT col1, TotalCol2
FROM v_SummarizedData;

SHOW TABLES;

SELECT datetime, level, event_id
FROM system_log
LIMIT 10;

SELECT unix_timestamp(datetime, 'dd/MM/yyyy hh:mm:ss') AS time_stamp, level, event_id
FROM system_log;

SELECT from_unixtime(unix_timestamp(date, 'dd/MM/yyyy hh:mm:ss')) AS datetime, level, event_id
FROM system_log;

CREATE VIEW v_system_log
AS
SELECT from_unixtime(unix_timestamp(date, 'dd/MM/yyyy hh:mm:ss')) AS datetime, level, event_id
FROM system_log;